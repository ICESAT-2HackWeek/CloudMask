{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of model\n",
    "\n",
    "The model we'll start with is based on [Hastie and Lee](https://web.stanford.edu/~hastie/Papers/structmgm.pdf). It fits a certain type of \n",
    "*undirected* graphical model to a sample of random variables of mixed type (discrete or continuous).\n",
    "\n",
    "It has the advantage that it also selects which *interactions* amongst the variables seem most relevant.\n",
    "\n",
    "Our data is\n",
    "$$\n",
    "X = \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and each sample $X_i$ is of mixed type -- in `numpy / pandas` terms we can think of it having a `dtype` with some `np.float` fields as well as some `pandas.Categotical` fields.\n",
    "We think of having $p$ different variables so we can talk about $X_{ij}, 1 \\leq i \\leq n, 1 \\leq j \\leq p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case: binary random variables\n",
    "\n",
    "In this case $X_i \\in \\{0,1\\}^p$. The model here is a generalization of the Ising model. That is, each row is like a realization of an Ising model so we have a sample of realizations of Ising models.\n",
    "\n",
    "The model is parameterized here by a $p \\times p$ symmetric matrix $\\Sigma$\n",
    "and\n",
    "$$\n",
    "P_{\\Theta}(X_i=x) \\propto \\exp\\left(\\sum_{j,k} x_j x_k \\Sigma_{jk} \\right).\n",
    "$$\n",
    "\n",
    "Classical Ising models have a fixed graph, say with adjacency matrix $A$ and perhaps two parameters $(\\theta_1, \\theta_2): one for interaction and one affecting the overall mean (external field? not sure, not a physicist). In these models\n",
    "$$\n",
    "P_{(\\theta_1, \\theta_2)}(X_i=x) \\proptop \\exp \\left(\\theta_1 \\sum_j x_j + \\theta_2 \\sum_{j, k} x_j x_k A_{jk}\n",
    "$$\n",
    "so we can think of this having a $\\Theta$ with $\\theta_1$ on the diagonal and $\\theta_2/2$ off the diagonal.\n",
    "\n",
    "This model therefore has a parameter for *each edge*. Edges are selected by putting a penalty on each edge, i.e. an $\\ell_1$ penalty perhaps leaving the mean terms unpenalized:\n",
    "$$\n",
    "{\\cal P}(\\Theta) = \\lambda \\sum_{(j,k): j \\neq k} |\\Theta_{jk}|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedolikelihood\n",
    "\n",
    "To fit this model is complicated -- it is expensive to normalize the $\\propto$ in the likelihood so people, dating back to Besag in the 80s have used pseudolikelihood as the objective.\n",
    "\n",
    "This objective is the sum of all the conditional likelihoods for one of the variables given all the others. E.g. in our Ising model here, if we *knew* $\\Sigma$ then for any $X_{i,j}$ we know that $X_{i,j}$ is Bernoulli and given $X_{i,k}, k \\neq j$ we can work out the probability it is 1 or 0 based on $\\Sigma$. This is effectively the likelihood for a logistic regression. There is therefore a likelihood for each column $j$\n",
    "$$\n",
    "\\ell_j(\\Theta; X[:,j] | X[:,k], k \\neq j)\n",
    "$$\n",
    "and the pseudo-likelihood (sum of conditional negative log-likelihoods) is\n",
    "$$\n",
    "\\Theta \\mapsto \\sum_{j=1}^p \\ell_j(\\Theta; X[:,j] | X[:,k], k \\neq j).\n",
    "$$\n",
    "\n",
    "The penalized *pseudolikelihood$ is \n",
    "$$\n",
    "\\Theta \\mapsto \\left[\\sum_{j=1}^p \\ell_j(\\Theta; X[:,j] | X[:,k], k \\neq j)\\right] + {\\cal P}(\\Theta).\n",
    "$$\n",
    "\n",
    "This is an objective we can minimize as a function of $\\Theta$. For large enough values of $\\lambda$ the minimize, $\\Theta$ will be sparse off-diagonal. This is the *undirected* graph selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case: all continuous\n",
    "\n",
    "When each feature is continuous, a common model for the distribution would be Gaussian. This can be parameterized in terms of sufficient statistics $X$ and $XX^T$ with natural parameters $\\alpha$ and $\\Theta$, say. So, the density can be written as\n",
    "$$\n",
    "P_{\\alpha,\\Theta}(X_i=x) \\propto \\exp \\left(\\alpha^Tx - \\frac{1}{2} \\text{Tr}(\\Theta xx^T) \\right).\n",
    "$$\n",
    "This is of course the normal family but parametrized slightly differently.\n",
    "\n",
    "This is quite similar to the binary case (in the binary case we could suck the $\\alpha$ into the diagonal of $\\Theta$ but this doesn't work in the Gaussian case.\n",
    "\n",
    "In this case, for one of the columns $X[:,j]$ there is a pseudo-likelihood corresponding to predicting $X[:,j]$ as a function of $X[:,k], k \\neq j$. Each term in the this pseudo-likelihood looks like a linear regression loss function. Summing these terms gives a pseudo-likelihood that could be used to fit the *graphical LASSO* (e.g. `glasso` in `R`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting these together\n",
    "\n",
    "It is now not hard to see how to mix continuous and binary. Each binary `field` has a reference sample space of $\\{0,1\\}$ while each continuous one has a reference sample space $\\mathbb{R}$. Stringing all fields together gives a sample space\n",
    "$$\n",
    "\\{0,1\\}^{j:j \\in {\\cal B}} \\times \\mathbb{R}^{j: j \\in {\\cal C}}\n",
    "$$\n",
    "where ${\\cal B}$ are the binary fields in our `dtype` and ${\\cal C}$ are the floating type fields in our `dtype`. If we had categorical instead of binary then that field's $\\{0,1\\}$ would be replaced by $\\{1, \\dots, N_j\\}$ where $N_j$ is the number of categories for field $j$.\n",
    "\n",
    "We now have a symmetric $p \\times p$ *matrix* $\\Theta$ (it's not really a matrix) where each entry $\\Theta_{jk}$ models the interaction between field $j$ and field $k$ of the `dtype`. When $j$ or $k$ is categorical (including binary) the entry $\\Theta_{jk}$ is really a matrix. Concretely, suppose $N_j=3$ and $N_k=5$ then, if we were to fit a \n",
    "multinomial regression (the analog of logistic for categorical) of $X[:,j]$ on to $X[:,k]$ then there would be a $3 \\times 5$ matrix of parameters in that model. (Note that much software often will set some of these to 0 automatically for identification reasons -- in this model with the penalty it is common not to do this). Anyways, we see that $\\Theta_{jk}$ is really in $\\mathbb{R}^{3 \\times 5}$ and $\\Theta_{kj} = \\Theta_{jk}^T \\in \\mathbb{R}^{5 \\times 3}$.\n",
    "\n",
    "The (or, an) analog of the $\\ell_1$ penalty for the *matrix* $\\Theta_{jk}$ is the Frobenius norm -- this is what the authors propose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to separate regressions\n",
    "\n",
    "In order to understand the relationships between columns it is tempting to simply regress each $X[:,j]$ onto all the other columns. The total objective in this case would be a sum of negative log-likelihoods and would look a lot like our pseudo-likelihood. The difference is that the pseudo-likelihood assumes symmetry and ties the parameters together this way. The separate regression framework drops this requirement.\n",
    "\n",
    "That is, the total loss for separate regressions is the same as the pseudo-likelihood\n",
    "but the pseudo-likelihood has linear constraints enforcing symmetry of the $\\Theta$ \"matrix\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "The model can be fit by proximal gradient methods, so we really just have to compute the\n",
    "objective (as a function of $\\Theta$ with $X$ fixed) and its gradient.\n",
    "\n",
    "As each term in the pseudo-likelihood is like a regression (negative log-) likelihood it is enough to have appropriate regression losses for each node.\n",
    "\n",
    "The proximal step is essentially the group LASSO proximal step (though here the parameters are matrices rather than vectors). By appropriate `vec` operations we should be able to use a group LASSO map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "0. Create a *design matrix* `X` and a dict mapping fields of the dtype to columns. Should include a column of 1s. Continuous features enter as columns unchanged. Categorical features are coded using *full* representation, i.e. for a categorical with $N_j$ levels we want $N_j$ 1-hot columns in the design matrix. Our final $\\Theta$ parameter will have shape $(X.shape[1], X.shape[1])$ and will be symmetric but we will store as regular `ndarray`.\n",
    "\n",
    "1. Create a representation of the pseudo-likelihood that can compute\n",
    "the maps\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Theta & \\mapsto \\sum_{j=1}^p \\ell_j\\left(\\Theta; X[:,j] | X[:,k], k \\neq j \\right) \\\\\n",
    "\\Theta & \\mapsto \\nabla_{\\Theta} \\sum_{j=1}^p \\ell_j\\left(\\Theta; X[:,j] | X[:,k], k \\neq j \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Note that $\\ell_j$ depends only on $\\Theta[j]$ and differentiating with respect to\n",
    "a term like $\\Theta[j,k]$ will involve only the losses $\\ell_k$ and $\\ell_j$.\n",
    "\n",
    "2. Make a group LASSO penalty for the $\\Theta$ matrix that sets $\\infty$ penalty on the \"diagonal\" terms $\\Theta[j,j]$.\n",
    "\n",
    "3. We should be able to just write the loss as a sum of saturated losses for each *response* composed with `X.dot(Theta[j])`, i.e.\n",
    "\n",
    "     loss(theta) = sum([s_loss(X.dot(Theta[j])) for s_loss in saturated_losses])\n",
    "\n",
    "4. For a categorical variable with $N_j$ levels `X.dot(Theta[j])` should have shape `(n, N_j)` and this product should effectively zero out any *self* terms in the sum over the\n",
    "$\\sum_j N_j$ in the matrix product. I.e. I am imaging that feature $j$ has been allocated $N_j$ columns in $X$ (the usual 1-hot encoding of multinomials).\n",
    "\n",
    "5. If we can write out the objective with a single `X` matrix, then I think the $\\Theta[j,k]$ gradient will simply look at the `k` rows of the `j` loss plus the `j` rows of the `k` loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example\n",
    "\n",
    "Let's make a data frame with 7 features, 2 binary, 2 categorical and 3 continuous. We'll code the binary as multinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "D = pd.DataFrame({'B1': np.random.choice([0,1], size=500),\n",
    "                  'B2': np.random.choice([0,1], size=500),\n",
    "                  'M1': np.random.choice(list('abcd'), size=500),\n",
    "                  'M2': np.random.choice(list('defgh'), size=500),\n",
    "                  'C1': np.random.standard_normal(500),\n",
    "                  'C2': np.random.standard_normal(500)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_onehot(categorical_col):\n",
    "    labels = sorted(np.unique(categorical_col))\n",
    "    return np.equal.outer(np.asarray(categorical_col), labels)\n",
    "\n",
    "def make_design(df, desc):\n",
    "    design = [np.ones((df.shape[0], 1))] # column for intercept\n",
    "    new_desc = [('intercept', None, slice(0, 1))]\n",
    "    idx = 1\n",
    "    for col, dtype in desc:\n",
    "        if dtype in ['binary', 'categorical']:\n",
    "            design_col = make_onehot(df[col])\n",
    "            ncol = design_col.shape[1]\n",
    "            design.append(design_col)\n",
    "            new_desc.append((col, dtype, slice(idx, idx + ncol)))\n",
    "            idx += ncol\n",
    "        elif dtype == 'continuous': # a continuous one\n",
    "            new_desc.append((col, dtype, slice(idx, idx+1)))\n",
    "            design_col = np.asarray(df[col].copy())\n",
    "            # should we standardize?\n",
    "            # design_col -= design_col.mean()\n",
    "            # design_col /= design_col.std()\n",
    "            design_col.shape = (-1,1)\n",
    "            design.append(design_col)\n",
    "    return np.hstack(design), new_desc\n",
    "\n",
    "desc =  [('B1','binary'),\n",
    "         ('B2','binary'),\n",
    "         ('M1','categorical'),\n",
    "         ('M2','categorical'),\n",
    "         ('C1','continuous'),\n",
    "         ('C2','continuous')]\n",
    "\n",
    "X, desc = make_design(D, desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form saturated losses \n",
    "\n",
    "This will use the `regreg` package: `http://github.com/jonathan-taylor/regreg.git`\n",
    "\n",
    "For `binary` or `categorical` we will use multinomial loss, while for `continuous`\n",
    "we will use Gaussian loss (i.e. squared error). We probably want to rescale each loss a little based on the variability under the *null* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C2',\n",
       " 'continuous',\n",
       " slice(14, 15, None),\n",
       " <regreg.smooth.glm.gaussian_loglike at 0x11aeb4d00>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from regreg.smooth.mglm import multinomial_loglike\n",
    "from regreg.smooth.glm import gaussian_loglike\n",
    "import regreg.api as rr\n",
    "\n",
    "new_desc = []\n",
    "for var, dtype, idx in desc:\n",
    "    if dtype in ['binary', 'categorical']:\n",
    "        response = X[:,idx]\n",
    "        new_desc.append((var, dtype, idx,\n",
    "                         multinomial_loglike(response.shape,\n",
    "                                             response)))\n",
    "    elif dtype == 'continuous':\n",
    "        response = np.squeeze(X[:,idx]) # it will be a 2D array\n",
    "        new_desc.append((var, dtype, idx,\n",
    "                         gaussian_loglike(response.shape,\n",
    "                                          response)))\n",
    "    else:\n",
    "        new_desc.append((var, dtype, idx, None))\n",
    "desc = new_desc\n",
    "desc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine losses into a smooth objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape(G):\n",
    "    if G.ndim == 1:\n",
    "        return G.reshape((-1, 1))\n",
    "    return G\n",
    "\n",
    "class full_loss(rr.smooth_atom):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 X,\n",
    "                 desc,\n",
    "                 coef=1.,\n",
    "                 offset=None,\n",
    "                 quadratic=None,\n",
    "                 case_weights=None,\n",
    "                 initial=None):\n",
    "        \n",
    "        (self.X, \n",
    "         self.desc) = (X, desc)\n",
    "\n",
    "        rr.smooth_atom.__init__(self,\n",
    "                                shape=(X.shape[1]**2),\n",
    "                                coef=1.,\n",
    "                                offset=offset,\n",
    "                                quadratic=quadratic,\n",
    "                                initial=initial)\n",
    "        \n",
    "        self._grad_buff = np.zeros((X.shape[1], \n",
    "                                    X.shape[1]))\n",
    "        \n",
    "    def smooth_objective(self, arg, mode='both'):\n",
    "        \n",
    "        arg = self.apply_offset(arg)\n",
    "        matrix_arg = arg.reshape((self._grad_buff.shape))\n",
    "        \n",
    "        \n",
    "        eta = natural_param = self.X.dot(matrix_arg) # again of shape as X\n",
    "        \n",
    "        if mode == 'func':\n",
    "            _f = [_loss.smooth_objective(np.squeeze(eta[:,_slice]), 'func') \n",
    "                  for _, _, _slice, _loss in self.desc if _loss is not None]\n",
    "            return self.scale(np.sum(_f))\n",
    "        elif mode == 'grad':\n",
    "            self._Gbuff\n",
    "            _g = [_reshape(_loss.smooth_objective(np.squeeze(eta[:,_slice]), \n",
    "                                                  'grad')) \n",
    "                  for _, _, _slice, _loss in self.desc if _loss is not None]\n",
    "            self._grad_buff[:,1:] = self.scale(self.X.T.dot(np.hstack(_g)))\n",
    "            self._grad_buff[:,0] = 0\n",
    "            return 0.5 * (self._grad_buff + self._grad_buff.T).reshape(-1)\n",
    "            \n",
    "        elif mode == 'both':\n",
    "            _f = [_loss.smooth_objective(np.squeeze(eta[:,_slice]), 'func') \n",
    "                  for _, _, _slice, _loss in self.desc if _loss is not None]\n",
    "            _g = [_reshape(_loss.smooth_objective(np.squeeze(eta[:,_slice]), \n",
    "                                                  'grad'))\n",
    "                  for _, _, _slice, _loss in self.desc if _loss is not None]\n",
    "            self._grad_buff[:,1:] = self.scale(self.X.T.dot(np.hstack(_g)))\n",
    "            self._grad_buff[:,0] = 0\n",
    "            return self.scale(np.sum(_f)), 0.5 * (self._grad_buff + self._grad_buff.T).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = full_loss(X, desc)\n",
    "F, G = loss.smooth_objective(np.zeros(loss.shape), 'both')\n",
    "G.shape           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct penalty\n",
    "\n",
    "The basic penalty is a group lasso for every \"block\" $\\Theta[k,j]$. We've flattened\n",
    "$\\Theta$ above so we can just use the usual group LASSO penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R, C = np.zeros((2, X.shape[1], X.shape[1]), np.int)\n",
    "var_names = list(D.columns) + ['intercept']\n",
    "\n",
    "for l, r in product(desc, desc):\n",
    "    l_slice = l[2]\n",
    "    r_slice = r[2]\n",
    "    R[l_slice][:,r_slice] = R[r_slice][:,l_slice] = var_names.index(r[0])\n",
    "    C[l_slice][:,r_slice] = C[r_slice][:,l_slice] = var_names.index(l[0])  \n",
    "I = np.transpose(np.array([R, C]), [1, 2, 0])\n",
    "I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(slice(1, 3, None), slice(3, 5, None))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B2_slice, M1_slice = desc[1][2], desc[2][2]\n",
    "B2_slice, M1_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [0, 0]]),\n",
       " array([[0, 0],\n",
       "        [0, 0]]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I[B2_slice][:,M1_slice][:,:,0], I[M1_slice][:,B2_slice][:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1],\n",
       "        [1, 1]]),\n",
       " array([[1, 1],\n",
       "        [1, 1]]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I[B2_slice][:,M1_slice][:,:,1], I[M1_slice][:,B2_slice][:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intercept', 'intercept'),\n",
       " ('B1', 'intercept'),\n",
       " ('B1', 'intercept'),\n",
       " ('B2', 'intercept'),\n",
       " ('B2', 'intercept'),\n",
       " ('M1', 'intercept'),\n",
       " ('M1', 'intercept'),\n",
       " ('M1', 'intercept'),\n",
       " ('M1', 'intercept'),\n",
       " ('M2', 'intercept'),\n",
       " ('M2', 'intercept'),\n",
       " ('M2', 'intercept'),\n",
       " ('M2', 'intercept'),\n",
       " ('M2', 'intercept'),\n",
       " ('C2', 'intercept'),\n",
       " ('B1', 'B1'),\n",
       " ('B1', 'intercept'),\n",
       " ('B1', 'B1'),\n",
       " ('B1', 'B1'),\n",
       " ('B1', 'B2'),\n",
       " ('B1', 'B2'),\n",
       " ('B1', 'M1'),\n",
       " ('B1', 'M1'),\n",
       " ('B1', 'M1'),\n",
       " ('B1', 'M1'),\n",
       " ('B1', 'M2'),\n",
       " ('B1', 'M2'),\n",
       " ('B1', 'M2'),\n",
       " ('B1', 'M2'),\n",
       " ('B1', 'M2'),\n",
       " ('B1', 'C2'),\n",
       " ('B1', 'B1'),\n",
       " ('B1', 'intercept'),\n",
       " ('B1', 'B1'),\n",
       " ('B1', 'B1'),\n",
       " ('B1', 'B2'),\n",
       " ('B1', 'B2'),\n",
       " ('B1', 'M1'),\n",
       " ('B1', 'M1'),\n",
       " ('B1', 'M1')]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = [tuple(sorted([var_names[r], var_names[c]])) for r, c in zip(R.reshape(-1), C.reshape(-1))]\n",
    "groups[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "ncol = {}\n",
    "for d in desc:\n",
    "    ncol[d[0]] = X[:,d[2]].shape[1]\n",
    "    \n",
    "for l, r in product(ncol.keys(), ncol.keys()):\n",
    "    weights[(l, r)] = np.sqrt(ncol[l] * ncol[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty = rr.group_lasso(list(groups), weights=weights, lagrange=1.)\n",
    "penalty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intercept': 500,\n",
       " 'B1': 500,\n",
       " 'B2': 500,\n",
       " 'M1': 500,\n",
       " 'M2': 500,\n",
       " 'C1': 500,\n",
       " 'C2': 500}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
